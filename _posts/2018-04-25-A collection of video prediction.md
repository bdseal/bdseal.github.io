---
layout:     post
title:      A collection of video prediction
subtitle:   Video Prediction, Time-Lapse Videos Generation and Dynamic Texture Synthesis
date:       2018-04-20
author:     Will Hsia
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - video prediction
---

## Related Works
- `SAVP`: Stochastic Adversarial Video Prediction. Arxiv 2018. [[HomePage](https://alexlee-gk.github.io/video_prediction/)] [[Paper](https://arxiv.org/abs/1804.01523)] [[Code](https://github.com/alexlee-gk/video_prediction)]
- `SV2P`: Stochastic Variational Video Prediction. ICLR 2018. [[Paper](https://arxiv.org/abs/1710.11252)] 
- `SVG-LP`: Stochastic Video Generation with a Learned Prior. Arxiv 2018. [[Paper](https://arxiv.org/abs/1802.07687)] 
- `CortexNet`: a Generic Network Family for Robust Visual Temporal Representations. Arxiv 2017. [[Paper](https://arxiv.org/pdf/1706.02735.pdf)] [[Code](https://github.com/atcold/pytorch-CortexNet)][[Project](https://engineering.purdue.edu/elab/CortexNet/)]
- `CDNA`: Unsupervised learning for physical interaction through video prediction. NIPS 2016. *Star 30,000+* [[NIPS 2016 Spotlight](https://www.youtube.com/watch?v=eYT_9xEu_8g)] [[Code](https://github.com/tensorflow/models/tree/master/research/video_prediction)] [[Paper](https://arxiv.org/abs/1605.07157)]

Time-Lapse Generation/Dynamic Texture Synthesis
- Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks. CVPR2018. [[Paper](https://arxiv.org/abs/1709.07592)]
- Two-Stream Convolutional Networks for Dynamic Texture Synthesis. CVPR2018.[[Paper](https://arxiv.org/abs/1706.06982)][[Code](https://github.com/ryersonvisionlab/two-stream-dyntex-synth)][[Project](https://ryersonvisionlab.github.io/two-stream-projpage/)]
- `DynTex`: A Comprehensive Database of Dynamic Textures. PRL 2010. [The DynTex Database](http://dyntex.univ-lr.fr/database.html)
- `STGConvNet`: Synthesizing dynamic patterns by spatial-temporal generative convnet. CVPR 2017. [[HomePage](https://github.com/zilongzheng/STGConvNet)] [[Paper](https://arxiv.org/abs/1606.00972)] [[Code](https://github.com/zilongzheng/STGConvNet/video_prediction)]


## Datasets
- `bair`: [BAIR robot pushing dataset](https://sites.google.com/view/sna-visual-mpc/). [[Citation](https://github.com/alexlee-gk/video_prediction/blob/master/data/bibtex/sna.txt)]
- `kth`: [KTH human actions dataset](http://www.nada.kth.se/cvap/actions/). [[Citation](https://github.com/alexlee-gk/video_prediction/blob/master/data/bibtex/kth.txt)]
- `DynTex`: [The DynTex Database](http://dyntex.univ-lr.fr/database.html)
- `5,000 time-lapse videos collected from Youtube`.*To Be Released*[[Paper](https://arxiv.org/abs/1709.07592)]
